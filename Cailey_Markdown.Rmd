---
output:
  pdf_document: default
  html_document: default
---
```{r}
library(readr)
full <- read_csv("diagnosing_AD_data.csv")
library(dplyr) #needed for the %>% function in the data subset loops
library(ggplot2)

demographics <- data_frame("Class" = c("Patients", "Control Group"), "Age" = c(71.5, 68.9), "Education" = c(10.8, 12.9), "#Women" = c(46,51), "#Men" = c(44,39))

library(knitr)
demographics %>%
  kable(col.names = c(" ", "Mean Age", "Mean Education (years)", "Number of Women", "Number of Men"))

library(lattice)
library(e1071)
library(caret)
```

```{r}
P_values <- data.frame(" " = c("Task 1", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task 7", "Task 8", "Task 9", "Task 10", "Task 11", "Task 12", "Task 13", "Task 14", "Task 15", "Task 16", "Task 17", "Task 18", "Task 19", "Task 20", "Task 21", "Task 22", "Task 23", "Task 24", "Task 25"), "Air Time" = c(0.370, "<0.001" , "<0.001", 0.009, "<0.001", "<0.001", 0.637, "<0.001", "<0.001", "<0.001", 0.618, 0.579, "<0.001", 0.643, "<0.001", "<0.001", 0.009, 0.310, 0.002, 0.007, "<0.001", 0.172, 0.484, "<0.001", 0.192), "Disp index" =c(0.003, "<0.001", 0.001, 0.05, 0.083, "<0.001", "<0.001", "<0.001", "<0.001", "<0.001", "<0.001", 0.002, 0.001, 0.025, "<0.001", 0.291, "<0.001", 0.802, 0.812, "<0.001", 0.386, "<0.001", "<0.001", 0.023, "<0.001"), "Max x extension" =c(0.445, 0.041, 0.069, 0.006, 0.004, 0.016, 0.032, 0.185, 0.020, 0.779, 0.363, 0.177, 0.561, 0.001, 0.054, 0.771, 0.258, 0.110, 0.004, 0.261, "<0.001", 0.001, 0.015, 0.268, 0.014), "Max y extension" =c(0.949, 0.013, 0.004, 0.002, 0.013, 0.324, 0.006, 0.160, 0.001, 0.114, 0.340, 0.959, 0.461, 0.007, 0.048, 0.679, 0.079, 0.875, 0.002, 0.453, 0.003, 0.115, 0.058, 0.345, "<0.001"), "Number of pendowns" = c(0.02, "<0.001", "<0.001", 0.001, "<0.001", 0.001, 0.002, "<0.001", "<0.001", "<0.001", "<0.001", 0.004, 0.004, 0.386, "<0.001", 0.155, "<0.001", 0.989, "<0.001", 0.001, 0.002, 0.001, "<0.001", 0.816, 0.073), "Paper Time" =c("<0.001", "<0.001", "<0.001", 0.001, 0.002, "<0.001", "<0.001", "<0.001", "<0.001", "<0.001", "<0.001", "<0.001", "<0.001", 0.001, "<0.001", 0.003, "<0.001", 0.323, 0.289, "<0.001", 0.095, "<0.001", "<0.001", "<0.001", "<0.001"), "Total Time" = c(0.088, "<0.001", "<0.001", 0.001, "<0.001", "<0.001", 0.981, "<0.001", "<0.001", "<0.001", 0.818, 0.351, "<0.001", 0.703, "<0.001", "<0.001", 0.001, 0.298, 0.002, 0.001, 0.001, 0.128, 0.143, "<0.001", 0.143))














P_values %>%
  kable(col.names = c("Task", "Air Time", "Dispersion Index", "Max x Extension", "Max y Extension", "Number of Pendowns", "Paper Time", "Total Time"))

```

```{r}
trial_parameters <- data.frame("Parameter" = c("Kernel", "Cost", "Gamma"), "Value" = c("Radial", 1, "Not specified (1)"))
trial_parameters %>%
  kable(col.names = c("Parameter", "Value"))
trial_confusion <- data.frame(" " = c("H", "P"), "H" = c(74, 50), "P" = c(11, 39))
trial_confusion %>%
  kable(col.names = c(" ", "H", "P"))

```

```{r}
a=1
b=6
c=15
access <- data.frame(matrix(nrow=174))

for (i in 1:25) {
  access <- access %>%
    mutate(full[a:(a+2)], full[b:(b+1)], full[c:(c+1)])
  
  a <- a+18
  b <- b+18
  c <- c+18
  
}

access <- access %>%
  mutate(full[451:452])

access <- subset(access, select = -(matrix.nrow...174.))

#separating the tasks

d <- 2

for (i in (1:25)) {
  df <- data.frame(access[1], access[d:(d+6)])
  df <- df %>%
    mutate(access[177])
  
  assign( paste("Task", i, sep = "_"), df)
  d <- d+7
}
```


```{r}
set.seed(2024)
library(caret)
svm_model <- function(task, kern){
  
  #divide data
  H <- which(task$class == "H") 
  P <- which(task$class == "P")
  
  n_train_H <- ceiling(0.8 * length(H))
  n_train_P <- ceiling(0.8 * length(P))
  
  train_H <- sample(H, size = n_train_H)
  train_P <- sample(P, size = n_train_P)
  
  train_both <- c(train_P, train_H)
  train_ID <- task[train_both, ]
  test_ID <- task[-train_both, ]
  
  #remove ID
  train <- subset(train_ID, select = -ID)
  test <- subset(test_ID, select = -ID)
  
  #grid search
  costs <- seq(from = 0.5, to = 1.5, by = 0.1)
  
  tune_cost <- tune(svm, as.factor(class) ~ ., data = train, ranges = list(cost = costs), kernel = kern, cross =5)
  
  best_cost <- tune_cost$best.parameters$cost
  
  svm_model <- svm(as.factor(class) ~ ., data = train, kernel = kern, cost = best_cost, gamma = 0.5)
  
  testing <- predict(svm_model, test)
  
  print(confusionMatrix(testing, as.factor(test$class)))
  
  summary(svm_model)
}

svm_model(full, 'linear')
```

```{r}
accuracy <- c(.8235, 0.7941, 0.9118, 0.7647, 0.8529, 0.7647, 0.7941, 0.7941, 0.7941, 0.9118)
CI <- c(.8235-.6547, 0.7941-0.621, 0.9118 - 0.7632, 0.7647-0.5883, 0.8529 - 0.6894, 0.7647 - 0.5883, 0.7941- 0.621, 0.7941 - 0.621, 0.7941 - 0.621, 0.9118 - 0.7632)
sensitivity <- c(0.7647, 0.8824, 0.9412, 0.7647, 0.8235, 0.7647, 0.8824, 0.8824, 0.8235, 0.8824)
specificity <- c(0.8824, 0.7059, 0.8824, 0.7647, 0.8824, 0.7647, 0.7059, 0.7059, 0.7647, 0.9412)
mean(accuracy)
mean(CI)
mean(sensitivity)
mean(specificity)
```






```{r}
full_model <- data_frame("Kernel" = "Linear", "Accuracy" = 79.41, "Sensitivity" = 82.35, "Specificity" = 76.47)

full_model %>%
  kable(col.names = c("Kernel", "Accuracy", "Specificity", "Sensitivity"))
```




